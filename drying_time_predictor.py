import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
# (â˜…) matplotlib, seaborn ì„í¬íŠ¸ ì œê±°
from sklearn.preprocessing import StandardScaler
from firebase_manager import RealtimeDatabaseManager
from heatmap_generator import create_correlation_heatmap  # (â˜…) ìƒˆ íŒŒì¼ì—ì„œ ì„í¬íŠ¸
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import matplotlib.pyplot as plt
from sklearn.model_selection import GroupShuffleSplit
"""
Firebase DBì—ì„œ ì›ë³¸(Raw) ì„¼ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ íœ´ì§€ê¸°ë¥¼ ì œê±°í•˜ê³  ê±´ì¡° ì„¸ì…˜ë³„ë¡œ ë¶„ë¦¬
ê° ì„¸ì…˜ì˜ ë°ì´í„°(í˜„ì¬ê°’, ë³€í™”ëŸ‰)ì™€ ë‚¨ì€ ê±´ì¡° ì‹œê°„(ì •ë‹µ)ì„ ê³„ì‚°í•˜ì—¬ AI ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  íŒŒì¼(.pkl)ë¡œ ì €ì¥
ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€, ìƒˆë¡œ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ì— ëŒ€í•œ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜ˆì¸¡(ì‹œë®¬ë ˆì´ì…˜)
"""


# (â˜…) Realtime Databaseìš© ë°ì´í„° ì¡°íšŒ í•¨ìˆ˜ (ìˆœì°¨ ì¡°íšŒ ë¡œì§)
def fetch_all_data_from_rtdb(key_path, db_url, base_data_path):
    """
    Realtime Databaseì—ì„œ base_data_path-1, -2, ... ê²½ë¡œì˜ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê°€ì ¸ì™€
    í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë³‘í•©í•˜ê³  ì •ë ¬í•©ë‹ˆë‹¤.
    """
    try:
        # 1. RealtimeDatabaseManager ê°ì²´ ìƒì„± (URL ì „ë‹¬ í•„ìˆ˜)
        rtdb_manager = RealtimeDatabaseManager(key_path, db_url)
        # 2. (firebase_manager.pyì— ì¶”ê°€ëœ í•¨ìˆ˜) ìˆœì°¨ì  ë°ì´í„° ì¡°íšŒ
        df = rtdb_manager.fetch_sequential_paths_as_dataframe(base_data_path)

        if df.empty:
            return pd.DataFrame()

        # ì‹œê°„ìˆœ ì •ë ¬ ë³´ì¥
        df.sort_values(by='timestamp', inplace=True)
        return df
    except Exception as e:
        print(f"RTDB ì „ì²´ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


# (â˜…) "ì‹¤ì‹œê°„ ì¶”ì„¸" ë° "ì§„ì§œ ê±´ì¡° ì™„ë£Œ" ë¡œì§ì´ ì ìš©ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜
# (â˜…) "ì‹¤ì‹œê°„ ì¶”ì„¸" ë° "ì§„ì§œ ê±´ì¡° ì™„ë£Œ" ë¡œì§ì´ ì ìš©ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_data_for_training(df_original,
                                 session_threshold_hours=1,
                                 dry_threshold_percent=1.0,  # (â˜…) ì¶”ê°€
                                 dry_stable_rows=10):  # (â˜…) ì¶”ê°€
    """
    (í•™ìŠµìš©) ì›ë³¸ ë°ì´í„°ë¥¼ "ì‹¤ì‹œê°„ ì¶”ì„¸" ì˜ˆì¸¡ ëª¨ë¸ìš©ìœ¼ë¡œ ê°€ê³µí•©ë‹ˆë‹¤.
    - ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ì„¸ì…˜ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    - (â˜…) ë‚®ì€ ìŠµë„ê°€ ì§€ì†ë˜ëŠ” ì‹œì ì„ 'ì§„ì§œ' ì¢…ë£Œ ì‹œì ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë°ì´í„°ë¥¼ ìë¦…ë‹ˆë‹¤.
    - (â˜…ìˆ˜ì •) ê±´ì¡°ê°€ ì™„ë£Œë˜ì§€ ì•Šì€(ì¤‘ë‹¨ëœ) ì„¸ì…˜ì€ í•™ìŠµì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
    - ê° ì„¸ì…˜ ë‚´ì—ì„œ 'ë‚¨ì€ ê±´ì¡° ì‹œê°„'ê³¼ 'ë³€í™”ëŸ‰' í”¼ì²˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if df_original.empty:
        return pd.DataFrame(), pd.Series(), []

    df = df_original.copy()

    # 1. ì‹¤ì œ ì»¬ëŸ¼ ì´ë¦„ìœ¼ë¡œ í”¼ì²˜ ê³„ì‚° ë° ì´ë¦„ í‘œì¤€í™”
    df['light_lux_avg'] = df['lux1']
    moist_cols = ['moisture_percent_1', 'moisture_percent_2', 'moisture_percent_3', 'moisture_percent_4']
    df['cloth_humidity'] = df[moist_cols].mean(axis=1)
    df = df.rename(columns={
        'temperature': 'ambient_temp',
        'humidity': 'ambient_humidity'
    })

    # 2. ì‹œê°„ìˆœ ì •ë ¬
    df = df.sort_values(by='timestamp').reset_index(drop=True)

    # 3. ì„¸ì…˜ ID ìƒì„±
    time_diff = df['timestamp'].diff().dt.total_seconds() / 3600
    df['session_id'] = (time_diff > session_threshold_hours).cumsum()

    print(f"ì´ {df['session_id'].nunique()}ê°œì˜ ê±´ì¡° ì„¸ì…˜ì„ ê°ì§€í–ˆìŠµë‹ˆë‹¤.")
    print(f" (â˜…) ê±´ì¡° ì™„ë£Œ ê¸°ì¤€: ìŠµë„ < {dry_threshold_percent}%ê°€ {dry_stable_rows}ê°œ í¬ì¸íŠ¸ ì—°ì† ìœ ì§€ ì‹œ")

    # 4. ê° ì„¸ì…˜ë³„ë¡œ "ì‹¤ì‹œê°„ í”¼ì²˜" ìƒì„±
    all_sessions_data = []
    for session_id in df['session_id'].unique():
        session_df = df[df['session_id'] == session_id].copy()

        # (â˜…) --- "ì§„ì§œ" ê±´ì¡° ì™„ë£Œ ì‹œì  íƒì§€ ---
        is_dry = session_df['cloth_humidity'] < dry_threshold_percent
        is_stable_dry = is_dry.rolling(window=dry_stable_rows).sum() >= dry_stable_rows
        stable_indices_loc = np.where(is_stable_dry)[0]  # .iloc ìœ„ì¹˜

        if len(stable_indices_loc) > 0:
            first_stable_end_iloc = stable_indices_loc[0]
            first_stable_start_iloc = first_stable_end_iloc - dry_stable_rows + 1
            true_end_timestamp = session_df.iloc[first_stable_start_iloc]['timestamp']

            print(f"  (ì„¸ì…˜ {session_id}) 'ì§„ì§œ' ê±´ì¡° ì™„ë£Œ ì‹œì  ê°ì§€: {true_end_timestamp}")

            # "ì™„ë£Œ ì´í›„ì˜ ë°ì´í„°(ìœ íœ´ ë°ì´í„°)ë¥¼ ìë¦„"
            session_df = session_df[session_df['timestamp'] <= true_end_timestamp].copy()

        else:
            # (â˜… ì¤‘ìš” ìˆ˜ì •) ê±´ì¡° ì™„ë£Œ ì¡°ê±´ì— ë„ë‹¬í•˜ì§€ ëª»í•œ ì„¸ì…˜ì€ í•™ìŠµì—ì„œ ì œì™¸
            print(f"  (ì„¸ì…˜ {session_id}) ì•ˆì •ëœ ê±´ì¡° ìƒíƒœë¥¼ ê°ì§€í•˜ì§€ ëª»í•¨. >> í•™ìŠµ ë°ì´í„°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤. <<")
            continue  # ì´ ì„¸ì…˜ì€ all_sessions_dataì— ì¶”ê°€í•˜ì§€ ì•Šê³  ê±´ë„ˆëœ€

        # 4-1. (y) íƒ€ê²Ÿ ë³€ìˆ˜ ê³„ì‚°: "ë‚¨ì€ ê±´ì¡° ì‹œê°„"
        end_time = session_df['timestamp'].max()
        session_df['remaining_time_minutes'] = (end_time - session_df['timestamp']).dt.total_seconds() / 60

        # 4-2. (X) ì‹¤ì‹œê°„ í”¼ì²˜ ê³„ì‚°: ë³€í™”ëŸ‰(delta)ê³¼ ì¶”ì„¸(trend)
        session_df['Î”humidity'] = session_df['cloth_humidity'].diff().fillna(0)
        session_df['Î”illumination'] = session_df['light_lux_avg'].diff().fillna(0)
        session_df['humidity_trend'] = session_df['cloth_humidity'].rolling(3).mean().bfill()

        all_sessions_data.append(session_df)

    # (ì˜ˆì™¸ ì²˜ë¦¬) ìœ íš¨í•œ ì„¸ì…˜ì´ í•˜ë‚˜ë„ ì—†ì„ ê²½ìš°
    if not all_sessions_data:
        print("ê²½ê³ : ê±´ì¡° ì™„ë£Œ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ì„¸ì…˜ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. ì„ê³„ê°’(DRY_THRESHOLD)ì„ í™•ì¸í•˜ì„¸ìš”.")
        return pd.DataFrame(), pd.Series(), []

    # 5. ëª¨ë“  ì„¸ì…˜ ë°ì´í„°ë¥¼ ë‹¤ì‹œ í•˜ë‚˜ë¡œ í•©ì¹¨
    processed_df = pd.concat(all_sessions_data, ignore_index=True)

    # 6. í•™ìŠµìš© X, y ë°ì´í„° ë¶„ë¦¬
    features = [
        'ambient_temp',
        'ambient_humidity',
        'light_lux_avg',
        'cloth_humidity',
        'Î”humidity',
        'Î”illumination',
        'humidity_trend'
    ]
    target = 'remaining_time_minutes'

    # í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§ (NaN ê°’ ë“± ì œì™¸)
    processed_df = processed_df.dropna(subset=features + [target])


    if processed_df.empty:
        print("ì „ì²˜ë¦¬ í›„ ë‚¨ì€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(), pd.Series(), []
    groups = processed_df['session_id']
    X = processed_df[features]
    y = processed_df[target]

    print("ì‹¤ì‹œê°„ ì¶”ì„¸ ê¸°ë°˜ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.")
    return X, y, features , groups

'''
def plot_prediction_results(y_true, y_pred):
    plt.figure(figsize=(10, 6))

    # ì‚°ì ë„ ê·¸ë¦¬ê¸°
    plt.scatter(y_true, y_pred, alpha=0.5, color='blue', label='Data Points')

    # ì •ë‹µ ë¼ì¸ (y = x)
    max_val = max(y_true.max(), y_pred.max())
    min_val = min(y_true.min(), y_pred.min())
    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Perfect Prediction')

    plt.title('Actual vs Predicted Remaining Time')
    plt.xlabel('Actual Remaining Time (min)')
    plt.ylabel('Predicted Remaining Time (min)')
    plt.legend()
    plt.grid(True)
    plt.show()
'''
# (â˜…) ìŠ¤ì¼€ì¼ëŸ¬(StandardScaler)ë„ í•¨ê»˜ ì €ì¥í•˜ë„ë¡ ìˆ˜ì •
def create_and_save_model(X, y,groups):
    """ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€"""
    if X.empty or y.empty:
        print("í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ì–´ ëª¨ë¸ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    print("\n--- ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° í•™ìŠµ ì‹œì‘ ---")

    # 1. ë°ì´í„° ë¶„ë¦¬ (ëœë¤ ë¶„í•  -> ê·¸ë£¹ ë¶„í• ë¡œ ë³€ê²½)
    # ì„¸ì…˜ ID(groups)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì…‹ì„ ë‚˜ëˆ”
    splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
    train_idx, test_idx = next(splitter.split(X, y, groups=groups))

    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_test = X.iloc[test_idx]
    y_test = y.iloc[test_idx]

    print(f"í•™ìŠµ ë°ì´í„° ê°œìˆ˜: {len(X_train)} (ì„¸ì…˜ {groups.iloc[train_idx].nunique()}ê°œ)")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: {len(X_test)} (ì„¸ì…˜ {groups.iloc[test_idx].nunique()}ê°œ)")

    # 2. ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 3. ëª¨ë¸ í•™ìŠµ (ê¸°ì¡´ê³¼ ë™ì¼)
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=415,
        learning_rate=0.16,
        max_depth=3,
        random_state=42,
        gamma=2.9987,
        min_child_weight=8.4125,
        subsample=0.53406,
        colsample_bytree= 0.602372
    )
    model.fit(X_train_scaled, y_train)
    # 4. (â˜…) ì„±ëŠ¥ í‰ê°€
    y_pred = model.predict(X_test_scaled)

    mae = mean_absolute_error(y_test, y_pred)

    # [ìˆ˜ì •] ì˜µì…˜ ëŒ€ì‹  ì§ì ‘ ê³„ì‚° (í˜¸í™˜ì„± í•´ê²°)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    r2 = r2_score(y_test, y_pred)
    print(f"\n[ëª¨ë¸ í‰ê°€ ê²°ê³¼]")
    print(f"1. í‰ê·  ì˜¤ì°¨ (MAE): {mae:.2f}ë¶„ (í‰ê· ì ìœ¼ë¡œ {mae:.2f}ë¶„ ì •ë„ í‹€ë¦¼)")
    print(f"2. RMSE: {rmse:.2f}ë¶„")
    print(f"3. ì •í™•ë„ (RÂ² Score): {r2:.4f} (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)")
    #plot_prediction_results(y_test, y_pred)
    # -------------------------------------------------------
    # 5. (ì„ íƒ) ì‹¤ì œ ì„œë¹„ìŠ¤ìš© ëª¨ë¸ì€ ì „ì²´ ë°ì´í„°ë¡œ ë‹¤ì‹œ ì¬í•™ìŠµí•´ì„œ ì €ì¥
    print("\n[ìµœì¢… ëª¨ë¸ ì €ì¥]")
    full_scaler = StandardScaler()
    X_scaled_full = full_scaler.fit_transform(X)  # ì „ì²´ ë°ì´í„° ì‚¬ìš©

    final_model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=415,
        learning_rate=0.16,
        max_depth=3,
        random_state=42,
        gamma=2.9987,
        min_child_weight=8.4125,
        subsample=0.53406,
        colsample_bytree= 0.602372
    )
    final_model.fit(X_scaled_full, y)

    joblib.dump(final_model, 'drying_model.pkl')
    joblib.dump(full_scaler, 'scaler.pkl')
    print("ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    return final_model


# (â˜…) "ì‹¤ì‹œê°„ ì¶”ì„¸" í”¼ì²˜ë¥¼ ìƒì„±í•˜ë„ë¡ ìˆ˜ì •ëœ ì˜ˆì¸¡ í•¨ìˆ˜
def make_features_for_prediction(current_session_df, features_list):
    """(ì˜ˆì¸¡ìš©) í˜„ì¬ ì„¸ì…˜ ë°ì´í„°ë¡œ ì‹¤ì‹œê°„ ì¶”ì„¸ í”¼ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # ìµœì†Œ 3ê°œ ë°ì´í„°ê°€ í•„ìš” (rolling(3) ë•Œë¬¸)
    if len(current_session_df) < 3:
        return None

    df = current_session_df.copy()

    # --- (â˜…) ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ ---

    # 1. ì‹¤ì œ ì»¬ëŸ¼ ì´ë¦„ìœ¼ë¡œ í”¼ì²˜ ê³„ì‚° ë° ì´ë¦„ í‘œì¤€í™”
    df['light_lux_avg'] = df['lux1']
    moist_cols = ['moisture_percent_1', 'moisture_percent_2', 'moisture_percent_3', 'moisture_percent_4']
    df['cloth_humidity'] = df[moist_cols].mean(axis=1)
    df = df.rename(columns={
        'temperature': 'ambient_temp',
        'humidity': 'ambient_humidity'
    })
    # --- (â˜…) ìˆ˜ì •ëœ ë¶€ë¶„ ë ---

    # 2. ê°€ì¥ ë§ˆì§€ë§‰ 3ê°œ ë°ì´í„° ì¶”ì¶œ
    latest, prev_1, prev_2 = df.iloc[-1], df.iloc[-2], df.iloc[-3]

    # 3. ì‹¤ì‹œê°„ ì¶”ì„¸ í”¼ì²˜ ìˆ˜ë™ ê³„ì‚°
    delta_humidity = latest['cloth_humidity'] - prev_1['cloth_humidity']
    delta_illumination = latest['light_lux_avg'] - prev_1['light_lux_avg']
    humidity_trend = (latest['cloth_humidity'] + prev_1['cloth_humidity'] + prev_2['cloth_humidity']) / 3

    # 4. ëª¨ë¸ì´ í•™ìŠµí•œ ìˆœì„œëŒ€ë¡œ 2D ë°°ì—´ ìƒì„±
    # (ë°ì´í„°í”„ë ˆì„ì„ ì ì‹œ ë§Œë“¤ì–´ ìˆœì„œë¥¼ ë³´ì¥)
    temp_df = pd.DataFrame([{
        'ambient_temp': latest['ambient_temp'],
        'ambient_humidity': latest['ambient_humidity'],
        'light_lux_avg': latest['light_lux_avg'],
        'cloth_humidity': latest['cloth_humidity'],
        'Î”humidity': delta_humidity,
        'Î”illumination': delta_illumination,
        'humidity_trend': humidity_trend
    }])

    # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ í”¼ì²˜ ëª©ë¡(features_list) ìˆœì„œëŒ€ë¡œ ì •ë ¬
    features = temp_df[features_list]

    return features


# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ (ìˆ˜ì •ë¨: ì¤‘ê°„ ì§€ì  'íƒ€ì„ë¨¸ì‹ ' ì˜ˆì¸¡) ---
if __name__ == '__main__':
    # --- 0. ì„¤ì • ---
    FIREBASE_KEY_PATH = "firebase.json"
    DATABASE_URL = "https://smart-drying-rack-fe271-default-rtdb.firebaseio.com/"
    BASE_DATA_PATH = "drying-rack"
    DRYING_COMPLETE_THRESHOLD = 1.0

    SESSION_THRESHOLD_HOURS = 2.0
    DRY_THRESHOLD = 1.0
    DRY_STABLE_POINTS = 10

    # --- 1. í•™ìŠµ ë‹¨ê³„ ---
    print("--- RTDBì—ì„œ ì „ì²´ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì‹œì‘ ---")
    all_completed_data = fetch_all_data_from_rtdb(FIREBASE_KEY_PATH, DATABASE_URL, BASE_DATA_PATH)

    if not all_completed_data.empty:
        # ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ
        X, y, trained_features, groups = preprocess_data_for_training(
            all_completed_data.copy(),
            session_threshold_hours=SESSION_THRESHOLD_HOURS,
            dry_threshold_percent=DRY_THRESHOLD,
            dry_stable_rows=DRY_STABLE_POINTS
        )
        create_and_save_model(X, y, groups)
    else:
        print("ë°ì´í„°ê°€ ì—†ì–´ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        exit()

    print("\n" + "=" * 50 + "\n")

    # --- 2. ì‹œë®¬ë ˆì´ì…˜ ë‹¨ê³„ (ì¤‘ê°„ ì‹œì  í…ŒìŠ¤íŠ¸) ---
    print("--- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜ (ê³¼ê±° ì¤‘ê°„ ì‹œì  í…ŒìŠ¤íŠ¸) ---")

    try:
        loaded_model = joblib.load('drying_model.pkl')
        loaded_scaler = joblib.load('scaler.pkl')
        print("ì €ì¥ëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

        if not all_completed_data.empty:
            # (1) ì„¸ì…˜ ë¶„ë¦¬ (ë§ˆì§€ë§‰ ì„¸ì…˜ ì°¾ê¸°)
            df_sim = all_completed_data.copy().sort_values(by='timestamp')
            time_diff = df_sim['timestamp'].diff().dt.total_seconds() / 3600
            df_sim['session_id'] = (time_diff > SESSION_THRESHOLD_HOURS).cumsum()

            last_session_id = df_sim['session_id'].max()
            last_session_df = df_sim[df_sim['session_id'] == last_session_id].copy().reset_index(drop=True)

            if len(last_session_df) > 10:
                # ----------------------------------------------------------------
                # [â˜… í•µì‹¬ ìˆ˜ì •] ë§¨ ë(tail)ì´ ì•„ë‹ˆë¼, "ì¤‘ê°„ ì§€ì "ì„ ê°•ì œë¡œ ì„ íƒ
                # ----------------------------------------------------------------
                # ì˜ˆ: ì „ì²´ ë°ì´í„°ì˜ 50% ì§€ì  (í•œì°½ ê±´ì¡° ì¤‘ì¼ ë•Œ)
                test_index = len(last_session_df) // 2

                # ì›í•˜ì‹ ë‹¤ë©´ íŠ¹ì • ìŠµë„ ì‹œì (ì˜ˆ: 30% ì´í•˜ê°€ ë˜ëŠ” ìˆœê°„)ì„ ì°¾ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:
                # moist_cols = ['moisture_percent_1', 'moisture_percent_2', 'moisture_percent_3', 'moisture_percent_4']
                # mean_humidity = last_session_df[moist_cols].mean(axis=1)
                # test_index = (mean_humidity < 30.0).idxmax() # ìŠµë„ê°€ 30% ë°‘ìœ¼ë¡œ ë–¨ì–´ì§„ ì²« ìˆœê°„

                # í•´ë‹¹ ì‹œì ê¹Œì§€ì˜ ë°ì´í„° ì˜ë¼ë‚´ê¸° (ìµœê·¼ 3ê°œ ë°ì´í„° í•„ìš”)
                current_data_slice = last_session_df.iloc[test_index - 3: test_index + 1]  # ì—¬ìœ ìˆê²Œ ê°€ì ¸ì˜´

                # ì˜ˆì¸¡ì„ ìœ„í•œ ë§ˆì§€ë§‰ í–‰ ê¸°ì¤€ ì •ë³´
                latest_row = current_data_slice.iloc[-1]
                current_timestamp = latest_row['timestamp']
                start_time = last_session_df['timestamp'].iloc[0]  # ì„¸ì…˜ ì‹œì‘ ì‹œê°„

                # (2) ê²½ê³¼ ì‹œê°„ ê³„ì‚°
                elapsed_minutes = (current_timestamp - start_time).total_seconds() / 60

                # í˜„ì¬ ìŠµë„ í™•ì¸
                moist_cols = ['moisture_percent_1', 'moisture_percent_2', 'moisture_percent_3', 'moisture_percent_4']
                current_humidity = latest_row[moist_cols].mean()

                print(f"â± [íƒ€ì„ë¨¸ì‹  ì‘ë™] í˜„ì¬ ì‹œì : ì„¸ì…˜ ì‹œì‘ í›„ {int(elapsed_minutes)}ë¶„ ê²½ê³¼")
                print(f"ğŸ’§ í˜„ì¬ í‰ê·  ìŠµë„: {current_humidity:.1f}% (ê±´ì¡° ì§„í–‰ ì¤‘)")

                # (3) ì˜ˆì¸¡ ìˆ˜í–‰
                prediction_features = make_features_for_prediction(current_data_slice, trained_features)

                if prediction_features is not None:
                    scaled_features = loaded_scaler.transform(prediction_features)
                    predicted_remaining_time = loaded_model.predict(scaled_features)[0]
                    predicted_remaining_time = max(0, predicted_remaining_time)

                    # (4) ì‹¤ì œ ì •ë‹µ(Actual) ê³„ì‚° (ë¯¸ë˜ë¥¼ ë¯¸ë¦¬ í™•ì¸)
                    real_end_time = last_session_df['timestamp'].max()
                    actual_remaining_time = (real_end_time - current_timestamp).total_seconds() / 60

                    print("-" * 30)
                    print(f"âœ… AI ì˜ˆì¸¡ ë‚¨ì€ ì‹œê°„: {int(predicted_remaining_time)}ë¶„")
                    print(f"ğŸ‘€ ì‹¤ì œ ì •ë‹µ ë‚¨ì€ ì‹œê°„: {int(actual_remaining_time)}ë¶„")
                    print(f"ğŸ¯ ì˜¤ì°¨: {int(abs(predicted_remaining_time - actual_remaining_time))}ë¶„")
                    print("-" * 30)

                    # ìš”ì²­í•˜ì‹  í¬ë§· ì¶œë ¥
                    print(f"ì˜ˆì¸¡ì‹œê°„:{int(predicted_remaining_time)}(min)  ê²½ê³¼ì‹œê°„:{int(elapsed_minutes)}(min)")
                else:
                    print("ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (í”¼ì²˜ ìƒì„± ì‹¤íŒ¨).")

            else:
                print("ë§ˆì§€ë§‰ ì„¸ì…˜ì˜ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜ ë°œìƒ: {e}")