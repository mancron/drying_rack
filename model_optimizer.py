import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import mean_squared_error, r2_score
from bayes_opt import BayesianOptimization

# ê¸°ì¡´ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
# (ë§Œì•½ í•¨ìˆ˜ ì´ë¦„ì´ preprocess_data_independent_sensorsë¼ë©´ ê·¸ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”)
from drying_time_predictor import fetch_all_data_from_rtdb, preprocess_data_for_training

# --- ì„¤ì • ---
FIREBASE_KEY_PATH = "firebase.json"
DATABASE_URL = "https://smart-drying-rack-fe271-default-rtdb.firebaseio.com/"
BASE_DATA_PATH = "drying-rack"

# ì „ì—­ ë³€ìˆ˜ë¡œ ë°ì´í„° ì €ì¥ (ìµœì í™” í•¨ìˆ˜ê°€ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ)
X_global = None
y_global = None
groups_global = None


def xgb_evaluate(max_depth, learning_rate, n_estimators, gamma, min_child_weight, subsample, colsample_bytree):
    """
    ë² ì´ì§€ì•ˆ ìµœì í™”ê°€ í˜¸ì¶œí•  'ëª©ì  í•¨ìˆ˜'ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ê²€ì¦ ì„¸íŠ¸ì˜ ì ìˆ˜(R2)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global X_global, y_global, groups_global

    # 1. ì •ìˆ˜í˜• ë³€í™˜ (ë² ì´ì§€ì•ˆ ìµœì í™”ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì‹¤ìˆ˜í˜•ì„ ë„˜ê²¨ì£¼ë¯€ë¡œ ë³€í™˜ í•„ìš”)
    max_depth = int(max_depth)
    n_estimators = int(n_estimators)

    # 2. ëª¨ë¸ ìƒì„±
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_jobs=-1,  # CPU ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
        max_depth=max_depth,
        learning_rate=learning_rate,
        n_estimators=n_estimators,
        gamma=gamma,
        min_child_weight=min_child_weight,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        random_state=42
    )

    # 3. ë°ì´í„° ë¶„í•  (ì„¸ì…˜ ë‹¨ìœ„ ë¶„ë¦¬ - ê³¼ì í•© ë°©ì§€ í•„ìˆ˜!)
    splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)

    # groups_globalì´ ì—†ìœ¼ë©´ ì¼ë°˜ ë¶„í• , ìˆìœ¼ë©´ ê·¸ë£¹ ë¶„í• 
    if groups_global is not None:
        train_idx, test_idx = next(splitter.split(X_global, y_global, groups=groups_global))
    else:
        # ê·¸ë£¹ ì •ë³´ê°€ ì—†ì„ ê²½ìš° ëŒ€ë¹„ (ì˜ˆì™¸ ì²˜ë¦¬)
        from sklearn.model_selection import train_test_split
        train_idx, test_idx = train_test_split(list(range(len(X_global))), test_size=0.2, random_state=42)

    X_train, X_test = X_global.iloc[train_idx], X_global.iloc[test_idx]
    y_train, y_test = y_global.iloc[train_idx], y_global.iloc[test_idx]

    # 4. ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 5. í•™ìŠµ ë° í‰ê°€
    model.fit(X_train_scaled, y_train)
    preds = model.predict(X_test_scaled)

    # ì ìˆ˜ ë°˜í™˜ (R2 Scoreê°€ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
    # ë§Œì•½ RMSEë¥¼ ì¤„ì´ëŠ” ê²Œ ëª©í‘œë¼ë©´ return -rmse ê°’ì„ ë°˜í™˜í•´ì•¼ í•¨
    score = r2_score(y_test, preds)

    # ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´(ìŒìˆ˜) ìµœì í™”ì— ë°©í•´ë˜ë¯€ë¡œ ìµœì†Œí•œì˜ ê°’ ì²˜ë¦¬ (ì„ íƒì‚¬í•­)
    return max(score, -10)


def run_optimization():
    global X_global, y_global, groups_global

    print("--- 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ---")
    raw_df = fetch_all_data_from_rtdb(FIREBASE_KEY_PATH, DATABASE_URL, BASE_DATA_PATH)

    if raw_df.empty:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì „ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ (drying_time_predictor.pyì— ìˆëŠ” í•¨ìˆ˜ ì‚¬ìš©)
    # ì£¼ì˜: ë°˜í™˜ê°’ì´ 4ê°œì¸ì§€ í™•ì¸í•˜ì„¸ìš” (X, y, features, groups)
    try:
        X_global, y_global, features, groups_global = preprocess_data_for_training(
            raw_df,
            session_threshold_hours=2.0,
            dry_threshold_percent=1.0,
            dry_stable_rows=10
        )
    except ValueError:
        print("ì˜¤ë¥˜: drying_time_predictor.pyì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ê°€ 'groups'ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
        print("ì „ì²˜ë¦¬ í•¨ìˆ˜ì˜ return ë¬¸ì´ 'return X, y, features, groups' í˜•íƒœì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    print(f"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(X_global)}ê°œ ìƒ˜í”Œ")
    print("\n--- 2. ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œì‘ ---")

    # íƒìƒ‰í•  íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ ì„¤ì •
    pbounds = {
        'max_depth': (3, 10),  # ë‚˜ë¬´ì˜ ê¹Šì´ (ë„ˆë¬´ ê¹Šìœ¼ë©´ ê³¼ì í•©)
        'learning_rate': (0.01, 0.3),  # í•™ìŠµë¥ 
        'n_estimators': (100, 1000),  # ë‚˜ë¬´ì˜ ê°œìˆ˜
        'gamma': (0, 5),  # ê°€ì§€ì¹˜ê¸° ê¸°ì¤€
        'min_child_weight': (1, 10),  # ê´€ì¸¡ì¹˜ ìµœì†Œ ë¬´ê²Œ
        'subsample': (0.5, 1.0),  # ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨
        'colsample_bytree': (0.5, 1.0)  # í”¼ì²˜ ìƒ˜í”Œë§ ë¹„ìœ¨
    }

    optimizer = BayesianOptimization(
        f=xgb_evaluate,
        pbounds=pbounds,
        random_state=42,
        verbose=2
    )

    # ìµœì í™” ì‹¤í–‰ (init_points: ì´ˆê¸° ëœë¤ íƒìƒ‰ íšŸìˆ˜, n_iter: ìµœì í™” ë°˜ë³µ íšŸìˆ˜)
    # ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë©´ n_iterë¥¼ ì¤„ì´ì„¸ìš”.
    optimizer.maximize(init_points=5, n_iter=20)

    print("\n" + "=" * 50)
    print("ğŸ‰ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬!")
    print("=" * 50)
    best_params = optimizer.max['params']

    # ì •ìˆ˜í˜• íŒŒë¼ë¯¸í„°ëŠ” ë³´ê¸° ì¢‹ê²Œ ë³€í™˜
    best_params['max_depth'] = int(best_params['max_depth'])
    best_params['n_estimators'] = int(best_params['n_estimators'])

    for key, value in best_params.items():
        print(f"{key}: {value}")

    print(f"\nìµœê³  RÂ² ì ìˆ˜: {optimizer.max['target']:.4f}")
    print("=" * 50)
    print("ì´ì œ ìœ„ ê°’ë“¤ì„ drying_time_predictor.pyì˜ XGBRegressor() ì•ˆì— ë„£ì–´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.")


if __name__ == '__main__':
    run_optimization()